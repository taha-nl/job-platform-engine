[2023-12-16T20:41:19.859+0000] {processor.py:157} INFO - Started process (PID=1101) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:41:19.861+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:41:19.863+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:41:19.863+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:41:19.879+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:41:19.873+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:41:19.880+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:41:19.915+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.064 seconds
[2023-12-16T20:41:50.446+0000] {processor.py:157} INFO - Started process (PID=1151) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:41:50.448+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:41:50.449+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:41:50.448+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:41:50.483+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:41:50.474+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:41:50.484+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:41:50.555+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.129 seconds
[2023-12-16T20:42:20.962+0000] {processor.py:157} INFO - Started process (PID=1201) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:42:20.967+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:42:20.970+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:42:20.969+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:42:21.014+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:42:21.011+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:42:21.015+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:42:21.137+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.196 seconds
[2023-12-16T20:42:51.245+0000] {processor.py:157} INFO - Started process (PID=1257) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:42:51.247+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:42:51.248+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:42:51.248+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:42:51.259+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:42:51.256+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:42:51.260+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:42:51.291+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.051 seconds
[2023-12-16T20:45:48.183+0000] {processor.py:157} INFO - Started process (PID=45) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:45:48.191+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:45:48.209+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:45:48.208+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:45:48.296+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:45:48.293+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:45:48.296+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:45:48.379+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.216 seconds
[2023-12-16T20:46:18.930+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:46:18.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:46:18.936+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:46:18.936+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:46:18.956+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:46:18.953+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:46:18.957+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:46:18.990+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.069 seconds
[2023-12-16T20:46:49.338+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:46:49.339+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:46:49.340+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:46:49.340+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:46:49.372+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:46:49.370+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:46:49.373+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:46:49.398+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.066 seconds
[2023-12-16T20:47:19.779+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:47:19.783+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:47:19.789+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:47:19.786+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:47:19.816+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:47:19.814+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:47:19.817+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:47:19.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.081 seconds
[2023-12-16T20:47:50.190+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:47:50.195+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:47:50.197+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:47:50.196+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:47:50.216+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:47:50.213+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:47:50.217+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:47:50.261+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.092 seconds
[2023-12-16T20:48:20.616+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:20.632+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:48:20.640+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:48:20.636+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:20.703+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:48:20.689+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 40, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '7.3.x-scala2.12', 'num_workers': 2, 'node_type_id': 'i3.xlarge'}}
[2023-12-16T20:48:20.704+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:20.757+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.160 seconds
[2023-12-16T20:48:21.720+0000] {processor.py:157} INFO - Started process (PID=90) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:21.722+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:48:21.723+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:48:21.723+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:21.771+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:48:21.763+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:48:21.772+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:21.937+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.223 seconds
[2023-12-16T20:48:52.337+0000] {processor.py:157} INFO - Started process (PID=99) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:52.347+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:48:52.348+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:48:52.348+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:52.388+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:48:52.377+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:48:52.388+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:48:52.472+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.144 seconds
[2023-12-16T20:49:22.664+0000] {processor.py:157} INFO - Started process (PID=108) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:49:22.666+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:49:22.667+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:49:22.667+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:49:22.685+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:49:22.682+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:49:22.686+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:49:22.728+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.070 seconds
[2023-12-16T20:49:53.145+0000] {processor.py:157} INFO - Started process (PID=117) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:49:53.147+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:49:53.149+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:49:53.148+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:49:53.175+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:49:53.170+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:49:53.176+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:49:53.230+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.092 seconds
[2023-12-16T20:50:23.413+0000] {processor.py:157} INFO - Started process (PID=125) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:50:23.414+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:50:23.416+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:50:23.416+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:50:23.432+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:50:23.430+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:50:23.433+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:50:23.464+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.058 seconds
[2023-12-16T20:50:53.779+0000] {processor.py:157} INFO - Started process (PID=134) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:50:53.781+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:50:53.783+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:50:53.782+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:50:53.800+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:50:53.798+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:50:53.801+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:50:53.832+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.067 seconds
[2023-12-16T20:51:24.084+0000] {processor.py:157} INFO - Started process (PID=143) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:51:24.086+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:51:24.088+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:51:24.087+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:51:24.103+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:51:24.101+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:51:24.104+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:51:24.163+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.085 seconds
[2023-12-16T20:51:54.407+0000] {processor.py:157} INFO - Started process (PID=152) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:51:54.408+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:51:54.410+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:51:54.409+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:51:54.431+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:51:54.427+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:51:54.433+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:51:54.463+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.062 seconds
[2023-12-16T20:52:24.864+0000] {processor.py:157} INFO - Started process (PID=160) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:52:24.875+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:52:24.878+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:52:24.877+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:52:24.916+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:52:24.913+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:52:24.917+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:52:25.024+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.176 seconds
[2023-12-16T20:52:55.219+0000] {processor.py:157} INFO - Started process (PID=169) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:52:55.222+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:52:55.223+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:52:55.223+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:52:55.254+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:52:55.245+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:52:55.255+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:52:55.325+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.114 seconds
[2023-12-16T20:53:26.154+0000] {processor.py:157} INFO - Started process (PID=178) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:53:26.156+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:53:26.159+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:53:26.158+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:53:26.183+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:53:26.180+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:53:26.184+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:53:26.254+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.109 seconds
[2023-12-16T20:53:56.595+0000] {processor.py:157} INFO - Started process (PID=187) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:53:56.597+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:53:56.598+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:53:56.598+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:53:56.617+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:53:56.614+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:53:56.617+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:53:56.645+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.058 seconds
[2023-12-16T20:54:27.116+0000] {processor.py:157} INFO - Started process (PID=196) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:54:27.118+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:54:27.119+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:54:27.118+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:54:27.135+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:54:27.133+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:54:27.136+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:54:27.174+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.065 seconds
[2023-12-16T20:54:57.618+0000] {processor.py:157} INFO - Started process (PID=205) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:54:57.619+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:54:57.620+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:54:57.620+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:54:57.638+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:54:57.636+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:54:57.639+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:54:57.668+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.057 seconds
[2023-12-16T20:55:27.931+0000] {processor.py:157} INFO - Started process (PID=214) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:55:27.933+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:55:27.935+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:55:27.934+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:55:27.959+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:55:27.955+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:55:27.959+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:55:27.991+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.069 seconds
[2023-12-16T20:55:58.324+0000] {processor.py:157} INFO - Started process (PID=223) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:55:58.326+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:55:58.328+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:55:58.327+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:55:58.353+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:55:58.350+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:55:58.354+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:55:58.384+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.066 seconds
[2023-12-16T20:56:28.690+0000] {processor.py:157} INFO - Started process (PID=232) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:56:28.701+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:56:28.718+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:56:28.717+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:56:28.797+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:56:28.793+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:56:28.805+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:56:28.858+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.182 seconds
[2023-12-16T20:56:59.111+0000] {processor.py:157} INFO - Started process (PID=241) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:56:59.113+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:56:59.115+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:56:59.115+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:56:59.132+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:56:59.129+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:56:59.133+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:56:59.158+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.055 seconds
[2023-12-16T20:57:29.465+0000] {processor.py:157} INFO - Started process (PID=250) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:57:29.468+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:57:29.472+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:57:29.471+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:57:29.526+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:57:29.523+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:57:29.527+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:57:29.576+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.122 seconds
[2023-12-16T20:58:00.380+0000] {processor.py:157} INFO - Started process (PID=259) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:00.398+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:58:00.400+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:58:00.400+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:00.510+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:58:00.463+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 42, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:58:00.553+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:00.765+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.405 seconds
[2023-12-16T20:58:15.992+0000] {processor.py:157} INFO - Started process (PID=260) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:15.995+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:58:15.997+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:58:15.996+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:16.069+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:58:16.061+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:58:16.070+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:16.147+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.165 seconds
[2023-12-16T20:58:46.376+0000] {processor.py:157} INFO - Started process (PID=269) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:46.379+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:58:46.381+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:58:46.380+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:46.405+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:58:46.402+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:58:46.406+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:58:46.444+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.075 seconds
[2023-12-16T20:59:16.715+0000] {processor.py:157} INFO - Started process (PID=278) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:59:16.717+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:59:16.718+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:59:16.718+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:59:16.744+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:59:16.740+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:59:16.745+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:59:16.779+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.071 seconds
[2023-12-16T20:59:47.154+0000] {processor.py:157} INFO - Started process (PID=287) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T20:59:47.156+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T20:59:47.159+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:59:47.158+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:59:47.186+0000] {logging_mixin.py:154} INFO - [2023-12-16T20:59:47.183+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T20:59:47.186+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T20:59:47.235+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.092 seconds
[2023-12-16T21:00:17.720+0000] {processor.py:157} INFO - Started process (PID=295) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:00:17.723+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:00:17.725+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:00:17.724+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:00:17.750+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:00:17.746+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:00:17.753+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:00:17.796+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.085 seconds
[2023-12-16T21:00:48.114+0000] {processor.py:157} INFO - Started process (PID=303) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:00:48.140+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:00:48.142+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:00:48.141+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:00:48.181+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:00:48.173+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:00:48.183+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:00:48.323+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.234 seconds
[2023-12-16T21:01:18.745+0000] {processor.py:157} INFO - Started process (PID=312) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:01:18.760+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:01:18.762+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:01:18.761+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:01:18.818+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:01:18.810+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:01:18.821+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:01:18.925+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.223 seconds
[2023-12-16T21:01:49.134+0000] {processor.py:157} INFO - Started process (PID=321) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:01:49.138+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:01:49.139+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:01:49.139+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:01:49.169+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:01:49.165+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:01:49.170+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:01:49.232+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.111 seconds
[2023-12-16T21:02:19.604+0000] {processor.py:157} INFO - Started process (PID=330) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:02:19.607+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:02:19.608+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:02:19.608+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:02:19.626+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:02:19.624+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:02:19.627+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:02:19.680+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.094 seconds
[2023-12-16T21:02:50.007+0000] {processor.py:157} INFO - Started process (PID=339) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:02:50.009+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:02:50.010+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:02:50.010+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:02:50.032+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:02:50.028+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:02:50.033+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:02:50.063+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.063 seconds
[2023-12-16T21:03:20.448+0000] {processor.py:157} INFO - Started process (PID=348) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:03:20.450+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:03:20.451+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:20.450+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:03:20.470+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:20.467+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 49, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'cluster_id': {'spark_version': '13.3.x-scala2.12', 'num_workers': 0, 'node_type_id': 'Standard_DS3_v2'}}
[2023-12-16T21:03:20.471+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:03:20.504+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.062 seconds
[2023-12-16T21:03:39.610+0000] {processor.py:157} INFO - Started process (PID=357) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:03:39.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:03:39.614+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:39.613+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:03:39.664+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:03:40.771+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:40.771+0000] {manager.py:499} INFO - Created Permission View: %s
[2023-12-16T21:03:40.796+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:40.796+0000] {manager.py:499} INFO - Created Permission View: %s
[2023-12-16T21:03:40.812+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:40.811+0000] {manager.py:499} INFO - Created Permission View: %s
[2023-12-16T21:03:40.813+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:40.813+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:03:40.870+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:40.870+0000] {dag.py:2963} INFO - Creating ORM DAG for databricks_dag
[2023-12-16T21:03:40.925+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:03:40.924+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-15T00:00:00+00:00, run_after=2023-12-16T00:00:00+00:00
[2023-12-16T21:03:40.977+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 1.374 seconds
[2023-12-16T21:04:11.279+0000] {processor.py:157} INFO - Started process (PID=366) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:04:11.281+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:04:11.284+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:04:11.282+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:04:11.317+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:04:11.406+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:04:11.406+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:04:11.469+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:04:11.469+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:04:11.511+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.242 seconds
[2023-12-16T21:04:41.573+0000] {processor.py:157} INFO - Started process (PID=375) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:04:41.576+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:04:41.584+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:04:41.583+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:04:41.625+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:04:41.716+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:04:41.716+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:04:42.041+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:04:42.041+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:04:42.154+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.594 seconds
[2023-12-16T21:05:12.252+0000] {processor.py:157} INFO - Started process (PID=384) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:05:12.255+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:05:12.257+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:05:12.256+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:05:12.292+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:05:12.385+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:05:12.385+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:05:12.451+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:05:12.451+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:05:12.501+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.258 seconds
[2023-12-16T21:05:43.043+0000] {processor.py:157} INFO - Started process (PID=393) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:05:43.046+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:05:43.054+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:05:43.054+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:05:43.109+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:05:43.264+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:05:43.264+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:05:43.384+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:05:43.383+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:05:43.787+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.794 seconds
[2023-12-16T21:06:15.075+0000] {processor.py:157} INFO - Started process (PID=402) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:06:15.193+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:06:15.265+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:06:15.260+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:06:17.039+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:06:18.219+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:06:18.219+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:06:18.696+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:06:18.695+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:06:19.294+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 4.520 seconds
[2023-12-16T21:06:50.450+0000] {processor.py:157} INFO - Started process (PID=411) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:06:50.489+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:06:50.498+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:06:50.497+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:06:50.547+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:06:50.781+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:06:50.780+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:06:51.103+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:06:51.102+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:06:51.255+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.825 seconds
[2023-12-16T21:07:21.654+0000] {processor.py:157} INFO - Started process (PID=421) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:07:21.657+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:07:21.662+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:07:21.661+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:07:21.700+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:07:21.778+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:07:21.777+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:07:21.842+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:07:21.841+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:07:21.888+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.241 seconds
[2023-12-16T21:07:52.450+0000] {processor.py:157} INFO - Started process (PID=430) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:07:52.452+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:07:52.453+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:07:52.453+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:07:52.475+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:07:52.556+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:07:52.555+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:07:52.600+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:07:52.600+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:07:52.624+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.180 seconds
[2023-12-16T21:08:22.788+0000] {processor.py:157} INFO - Started process (PID=439) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:08:22.790+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:08:22.791+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:08:22.790+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:08:22.811+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:08:22.864+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:08:22.864+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:08:22.917+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:08:22.917+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:08:22.943+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.161 seconds
[2023-12-16T21:08:53.035+0000] {processor.py:157} INFO - Started process (PID=448) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:08:53.037+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:08:53.038+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:08:53.038+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:08:53.059+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:08:53.100+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:08:53.100+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:08:53.138+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:08:53.138+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:08:53.165+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.136 seconds
[2023-12-16T21:09:23.381+0000] {processor.py:157} INFO - Started process (PID=457) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:09:23.382+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:09:23.384+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:09:23.383+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:09:23.408+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:09:23.461+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:09:23.461+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:09:23.498+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:09:23.498+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:09:23.527+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.152 seconds
[2023-12-16T21:09:47.626+0000] {processor.py:157} INFO - Started process (PID=467) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:09:47.628+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:09:47.629+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:09:47.629+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:09:47.653+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:09:47.826+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:09:47.826+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:09:47.860+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:09:47.860+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:09:47.938+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.317 seconds
[2023-12-16T21:10:18.028+0000] {processor.py:157} INFO - Started process (PID=476) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:10:18.029+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:10:18.031+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:10:18.030+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:10:18.055+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:10:18.132+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:10:18.132+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:10:18.176+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:10:18.175+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:10:18.203+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.182 seconds
[2023-12-16T21:10:48.305+0000] {processor.py:157} INFO - Started process (PID=484) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:10:48.306+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:10:48.308+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:10:48.307+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:10:48.327+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:10:48.377+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:10:48.377+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:10:48.472+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:10:48.472+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:10:48.517+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.218 seconds
[2023-12-16T21:11:18.665+0000] {processor.py:157} INFO - Started process (PID=493) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:11:18.667+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:11:18.668+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:11:18.668+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:11:18.696+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:11:18.763+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:11:18.762+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:11:18.810+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:11:18.810+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:11:18.919+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.259 seconds
[2023-12-16T21:11:48.973+0000] {processor.py:157} INFO - Started process (PID=502) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:11:48.975+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:11:48.976+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:11:48.975+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:11:48.995+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:11:49.038+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:11:49.037+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:11:49.074+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:11:49.074+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:11:49.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.134 seconds
[2023-12-16T21:12:19.359+0000] {processor.py:157} INFO - Started process (PID=511) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:12:19.361+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:12:19.362+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:12:19.362+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:12:19.383+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:12:19.422+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:12:19.422+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:12:19.459+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:12:19.459+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:12:19.485+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.134 seconds
[2023-12-16T21:12:49.675+0000] {processor.py:157} INFO - Started process (PID=520) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:12:49.677+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:12:49.678+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:12:49.678+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:12:49.698+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:12:49.746+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:12:49.746+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:12:49.785+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:12:49.785+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:12:49.812+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.143 seconds
[2023-12-16T21:13:20.063+0000] {processor.py:157} INFO - Started process (PID=529) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:13:20.065+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:13:20.066+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:13:20.066+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:13:20.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:13:20.136+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:13:20.136+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:13:20.180+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:13:20.180+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:13:20.206+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.150 seconds
[2023-12-16T21:13:50.537+0000] {processor.py:157} INFO - Started process (PID=537) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:13:50.538+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:13:50.540+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:13:50.539+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:13:50.561+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:13:50.609+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:13:50.609+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:13:50.646+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:13:50.646+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:13:50.675+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.152 seconds
[2023-12-16T21:14:20.938+0000] {processor.py:157} INFO - Started process (PID=546) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:14:20.941+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:14:20.943+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:14:20.943+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:14:20.976+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:14:21.040+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:14:21.040+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:14:21.078+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:14:21.077+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:14:21.132+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.199 seconds
[2023-12-16T21:14:51.363+0000] {processor.py:157} INFO - Started process (PID=555) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:14:51.366+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:14:51.367+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:14:51.367+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:14:51.389+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:14:51.431+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:14:51.431+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:14:51.470+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:14:51.469+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:14:51.497+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.142 seconds
[2023-12-16T21:15:21.857+0000] {processor.py:157} INFO - Started process (PID=563) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:15:21.859+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:15:21.860+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:15:21.860+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:15:21.886+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:15:21.929+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:15:21.929+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:15:21.967+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:15:21.967+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:15:21.993+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.145 seconds
[2023-12-16T21:15:52.300+0000] {processor.py:157} INFO - Started process (PID=572) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:15:52.302+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:15:52.304+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:15:52.303+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:15:52.337+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:15:52.396+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:15:52.395+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:15:52.438+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:15:52.438+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:15:52.474+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.181 seconds
[2023-12-16T21:16:22.721+0000] {processor.py:157} INFO - Started process (PID=580) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:16:22.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:16:22.728+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:16:22.728+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:16:22.748+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:16:22.791+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:16:22.791+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:16:22.832+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:16:22.832+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:16:22.859+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.150 seconds
[2023-12-16T21:16:53.163+0000] {processor.py:157} INFO - Started process (PID=589) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:16:53.179+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:16:53.183+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:16:53.182+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:16:53.214+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:16:53.277+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:16:53.277+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:16:53.349+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:16:53.348+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:16:53.376+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.220 seconds
[2023-12-16T21:17:23.632+0000] {processor.py:157} INFO - Started process (PID=598) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:17:23.634+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:17:23.635+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:17:23.635+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:17:23.657+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:17:23.712+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:17:23.711+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:17:23.770+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:17:23.770+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:17:23.833+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.209 seconds
[2023-12-16T21:17:54.177+0000] {processor.py:157} INFO - Started process (PID=606) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:17:54.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:17:54.181+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:17:54.181+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:17:54.204+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:17:54.250+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:17:54.250+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:17:54.293+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:17:54.293+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:17:54.322+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.152 seconds
[2023-12-16T21:18:24.677+0000] {processor.py:157} INFO - Started process (PID=615) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:18:24.679+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:18:24.682+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:18:24.681+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:18:24.710+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:18:24.772+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:18:24.772+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:18:24.818+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:18:24.817+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:18:24.852+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.183 seconds
[2023-12-16T21:18:48.009+0000] {processor.py:157} INFO - Started process (PID=622) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:18:48.011+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:18:48.012+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:18:48.012+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:18:48.038+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:18:48.084+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:18:48.084+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:18:48.125+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:18:48.125+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:18:48.156+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.154 seconds
[2023-12-16T21:19:18.449+0000] {processor.py:157} INFO - Started process (PID=631) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:19:18.451+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:19:18.452+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:19:18.452+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:19:18.471+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:19:18.512+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:19:18.512+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:19:18.550+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:19:18.550+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:19:18.576+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.132 seconds
[2023-12-16T21:19:48.732+0000] {processor.py:157} INFO - Started process (PID=640) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:19:48.734+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:19:48.736+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:19:48.735+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:19:48.758+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:19:48.803+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:19:48.803+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:19:48.864+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:19:48.864+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:19:48.915+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.188 seconds
[2023-12-16T21:20:19.178+0000] {processor.py:157} INFO - Started process (PID=649) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:20:19.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:20:19.181+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:20:19.181+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:20:19.258+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:20:19.388+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:20:19.387+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:20:19.470+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:20:19.470+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:20:19.494+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.323 seconds
[2023-12-16T21:20:49.700+0000] {processor.py:157} INFO - Started process (PID=657) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:20:49.704+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:20:49.707+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:20:49.706+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:20:49.733+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:20:50.057+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:20:50.057+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:20:50.109+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:20:50.108+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:20:50.144+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.459 seconds
[2023-12-16T21:21:20.214+0000] {processor.py:157} INFO - Started process (PID=665) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:21:20.216+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:21:20.218+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:21:20.218+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:21:20.249+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:21:20.299+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:21:20.298+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:21:20.337+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:21:20.336+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:21:20.366+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.160 seconds
[2023-12-16T21:21:50.480+0000] {processor.py:157} INFO - Started process (PID=674) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:21:50.485+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:21:50.492+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:21:50.491+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:21:50.584+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:21:50.647+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:21:50.647+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:21:50.706+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:21:50.706+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:21:50.734+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.265 seconds
[2023-12-16T21:22:20.791+0000] {processor.py:157} INFO - Started process (PID=682) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:22:20.798+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:22:20.801+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:22:20.801+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:22:20.865+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:22:20.961+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:22:20.961+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:22:21.069+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:22:21.069+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:22:21.114+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.339 seconds
[2023-12-16T21:22:51.269+0000] {processor.py:157} INFO - Started process (PID=690) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:22:51.291+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:22:51.292+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:22:51.291+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:22:51.323+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:22:51.374+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:22:51.374+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:22:51.413+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:22:51.413+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:22:51.439+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.177 seconds
[2023-12-16T21:23:00.334+0000] {processor.py:157} INFO - Started process (PID=692) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:23:00.336+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:23:00.338+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:23:00.338+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:23:00.367+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:23:00.418+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:23:00.418+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:23:00.470+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:23:00.470+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:23:00.503+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.177 seconds
[2023-12-16T21:23:30.957+0000] {processor.py:157} INFO - Started process (PID=700) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:23:30.959+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:23:30.960+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:23:30.960+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:23:30.983+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:23:31.033+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:23:31.033+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:23:31.074+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:23:31.074+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:23:31.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.156 seconds
[2023-12-16T21:24:01.503+0000] {processor.py:157} INFO - Started process (PID=709) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:24:01.505+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:24:01.506+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:24:01.505+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:24:01.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:24:01.581+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:24:01.581+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:24:01.623+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:24:01.623+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:24:01.650+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.154 seconds
[2023-12-16T21:24:31.803+0000] {processor.py:157} INFO - Started process (PID=718) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:24:31.805+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:24:31.806+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:24:31.806+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:24:31.833+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:24:31.886+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:24:31.886+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:24:31.930+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:24:31.930+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:24:31.964+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.170 seconds
[2023-12-16T21:25:02.281+0000] {processor.py:157} INFO - Started process (PID=727) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:25:02.282+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:25:02.284+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:25:02.284+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:25:02.311+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:25:02.386+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:25:02.385+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:25:02.442+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:25:02.442+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:25:02.501+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.229 seconds
[2023-12-16T21:25:33.266+0000] {processor.py:157} INFO - Started process (PID=736) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:25:33.268+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:25:33.277+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:25:33.270+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:25:33.309+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:25:33.436+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:25:33.436+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:25:33.642+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:25:33.640+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-15T00:00:00+00:00, run_after=2023-12-16T00:00:00+00:00
[2023-12-16T21:25:33.764+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.504 seconds
[2023-12-16T21:26:03.835+0000] {processor.py:157} INFO - Started process (PID=744) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:26:03.837+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:26:03.840+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:26:03.840+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:26:03.879+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:26:03.934+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:26:03.933+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:26:03.973+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:26:03.973+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-15T00:00:00+00:00, run_after=2023-12-16T00:00:00+00:00
[2023-12-16T21:26:04.003+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.174 seconds
[2023-12-16T21:26:34.429+0000] {processor.py:157} INFO - Started process (PID=753) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:26:34.450+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:26:34.451+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:26:34.451+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:26:34.573+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:26:34.727+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:26:34.722+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:26:35.019+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:26:35.019+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-15T00:00:00+00:00, run_after=2023-12-16T00:00:00+00:00
[2023-12-16T21:26:35.269+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.878 seconds
[2023-12-16T21:27:05.529+0000] {processor.py:157} INFO - Started process (PID=762) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:27:05.531+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:27:05.533+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:27:05.533+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:27:05.603+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:27:05.696+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:27:05.695+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:27:05.754+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:27:05.754+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:27:05.869+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.351 seconds
[2023-12-16T21:27:36.858+0000] {processor.py:157} INFO - Started process (PID=770) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:27:36.860+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:27:36.862+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:27:36.861+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:27:36.885+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:27:36.935+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:27:36.935+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:27:36.971+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:27:36.971+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:27:36.997+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.145 seconds
[2023-12-16T21:28:07.377+0000] {processor.py:157} INFO - Started process (PID=779) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:28:07.379+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:28:07.382+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:28:07.381+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:28:07.407+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:28:07.448+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:28:07.448+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:28:07.488+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:28:07.488+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:28:07.513+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.144 seconds
[2023-12-16T21:28:37.873+0000] {processor.py:157} INFO - Started process (PID=788) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:28:37.875+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:28:37.876+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:28:37.876+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:28:37.897+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:28:37.941+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:28:37.941+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:28:37.987+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:28:37.986+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:28:38.017+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.153 seconds
[2023-12-16T21:29:08.196+0000] {processor.py:157} INFO - Started process (PID=797) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:29:08.198+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:29:08.200+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:29:08.198+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:29:08.227+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:29:08.282+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:29:08.282+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:29:08.325+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:29:08.325+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:29:08.359+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.169 seconds
[2023-12-16T21:29:38.615+0000] {processor.py:157} INFO - Started process (PID=805) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:29:38.617+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:29:38.619+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:29:38.618+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:29:38.638+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:29:38.676+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:29:38.676+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:29:38.713+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:29:38.713+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:29:38.741+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.133 seconds
[2023-12-16T21:30:08.850+0000] {processor.py:157} INFO - Started process (PID=814) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:30:08.854+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:30:08.856+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:30:08.856+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:30:08.886+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:30:08.962+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:30:08.962+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:30:09.007+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:30:09.007+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:30:09.043+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.201 seconds
[2023-12-16T21:30:39.197+0000] {processor.py:157} INFO - Started process (PID=823) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:30:39.199+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:30:39.201+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:30:39.201+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:30:39.231+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:30:39.294+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:30:39.293+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:30:39.336+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:30:39.336+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:30:39.387+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.199 seconds
[2023-12-16T21:31:09.515+0000] {processor.py:157} INFO - Started process (PID=831) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:31:09.517+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:31:09.518+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:31:09.518+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:31:09.540+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:31:09.618+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:31:09.618+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:31:09.699+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:31:09.698+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:31:09.742+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.233 seconds
[2023-12-16T21:31:39.923+0000] {processor.py:157} INFO - Started process (PID=840) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:31:39.937+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:31:39.944+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:31:39.939+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:31:39.977+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:31:40.052+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:31:40.052+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:31:40.113+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:31:40.112+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:31:40.142+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.230 seconds
[2023-12-16T21:32:10.245+0000] {processor.py:157} INFO - Started process (PID=848) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:32:10.246+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:32:10.248+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:32:10.248+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:32:10.269+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:32:10.317+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:32:10.317+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:32:10.356+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:32:10.356+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:32:10.387+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.149 seconds
[2023-12-16T21:32:40.703+0000] {processor.py:157} INFO - Started process (PID=856) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:32:40.705+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:32:40.706+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:32:40.706+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:32:40.726+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:32:40.765+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:32:40.765+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:32:40.802+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:32:40.802+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:32:40.829+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.133 seconds
[2023-12-16T21:33:10.961+0000] {processor.py:157} INFO - Started process (PID=865) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:33:10.962+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:33:10.963+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:33:10.963+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:33:10.987+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:33:11.047+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:33:11.047+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:33:11.105+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:33:11.105+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:33:11.141+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.186 seconds
[2023-12-16T21:33:41.300+0000] {processor.py:157} INFO - Started process (PID=874) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:33:41.302+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:33:41.304+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:33:41.303+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:33:41.327+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:33:41.389+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:33:41.389+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:33:41.429+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:33:41.429+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:33:41.461+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.168 seconds
[2023-12-16T21:34:11.897+0000] {processor.py:157} INFO - Started process (PID=883) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:11.899+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:34:11.900+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:11.900+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:11.922+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:11.971+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:11.970+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:34:12.011+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:12.011+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:34:12.039+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.149 seconds
[2023-12-16T21:34:12.970+0000] {processor.py:157} INFO - Started process (PID=884) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:12.972+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:34:12.975+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:12.974+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:12.996+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:12.993+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 34, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'job_id': 751742865096775}
[2023-12-16T21:34:12.997+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:13.045+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.081 seconds
[2023-12-16T21:34:43.370+0000] {processor.py:157} INFO - Started process (PID=893) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:43.372+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:34:43.374+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:43.373+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:43.406+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:34:43.403+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 34, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'job_id': 751742865096775}
[2023-12-16T21:34:43.407+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:34:43.456+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.094 seconds
[2023-12-16T21:35:13.623+0000] {processor.py:157} INFO - Started process (PID=901) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:35:13.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:35:13.626+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:35:13.626+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:35:13.641+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:35:13.639+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 34, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'job_id': 751742865096775}
[2023-12-16T21:35:13.642+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:35:13.673+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.058 seconds
[2023-12-16T21:35:44.029+0000] {processor.py:157} INFO - Started process (PID=910) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:35:44.032+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:35:44.034+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:35:44.033+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:35:44.054+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:35:44.051+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 34, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'job_id': 751742865096775}
[2023-12-16T21:35:44.055+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:35:44.088+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.065 seconds
[2023-12-16T21:36:14.396+0000] {processor.py:157} INFO - Started process (PID=919) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:14.397+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:36:14.399+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:14.398+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:14.419+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:14.415+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/databricks_test.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/databricks_test.py", line 34, in <module>
    trigger_databricks_notebook = DatabricksSubmitRunOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/decorators.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/operators/databricks.py", line 274, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 794, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to DatabricksSubmitRunOperator (task_id: trigger_databricks_notebook). Invalid arguments were:
**kwargs: {'job_id': 751742865096775}
[2023-12-16T21:36:14.420+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:14.499+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.108 seconds
[2023-12-16T21:36:15.492+0000] {processor.py:157} INFO - Started process (PID=920) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:15.494+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:36:15.496+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:15.495+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:15.524+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:15.724+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:15.724+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:36:15.759+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:15.759+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:36:15.792+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.307 seconds
[2023-12-16T21:36:46.095+0000] {processor.py:157} INFO - Started process (PID=929) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:46.097+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:36:46.098+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:46.098+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:46.126+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:36:46.188+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:46.188+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:36:46.243+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:36:46.243+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-15T00:00:00+00:00, run_after=2023-12-16T00:00:00+00:00
[2023-12-16T21:36:46.282+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.196 seconds
[2023-12-16T21:37:16.520+0000] {processor.py:157} INFO - Started process (PID=938) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:37:16.522+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:37:16.524+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:37:16.523+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:37:16.543+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:37:16.587+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:37:16.587+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:37:16.627+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:37:16.627+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-15T00:00:00+00:00, run_after=2023-12-16T00:00:00+00:00
[2023-12-16T21:37:16.653+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.139 seconds
[2023-12-16T21:37:47.092+0000] {processor.py:157} INFO - Started process (PID=946) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:37:47.095+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:37:47.099+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:37:47.097+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:37:47.133+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:37:47.186+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:37:47.186+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:37:47.229+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:37:47.228+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:37:47.253+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.168 seconds
[2023-12-16T21:38:17.409+0000] {processor.py:157} INFO - Started process (PID=955) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:38:17.410+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:38:17.412+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:38:17.411+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:38:17.438+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:38:17.502+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:38:17.500+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:38:17.541+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:38:17.541+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:38:17.573+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.171 seconds
[2023-12-16T21:38:47.936+0000] {processor.py:157} INFO - Started process (PID=963) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:38:47.937+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:38:47.939+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:38:47.938+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:38:47.960+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:38:48.003+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:38:48.003+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:38:48.082+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:38:48.082+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:38:48.110+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.181 seconds
[2023-12-16T21:39:18.270+0000] {processor.py:157} INFO - Started process (PID=971) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:39:18.272+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:39:18.273+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:39:18.273+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:39:18.296+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:39:18.348+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:39:18.348+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:39:18.411+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:39:18.411+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:39:18.473+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.211 seconds
[2023-12-16T21:39:48.838+0000] {processor.py:157} INFO - Started process (PID=980) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:39:48.840+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:39:48.842+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:39:48.841+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:39:48.867+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:39:48.914+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:39:48.914+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:39:48.958+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:39:48.958+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:39:48.987+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.156 seconds
[2023-12-16T21:40:19.282+0000] {processor.py:157} INFO - Started process (PID=989) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:40:19.284+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:40:19.285+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:40:19.285+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:40:19.311+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:40:19.361+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:40:19.361+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:40:19.410+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:40:19.410+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:40:19.440+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.166 seconds
[2023-12-16T21:40:49.868+0000] {processor.py:157} INFO - Started process (PID=998) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:40:49.870+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:40:49.871+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:40:49.871+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:40:49.899+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:40:49.947+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:40:49.947+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:40:49.996+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:40:49.996+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:40:50.028+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.171 seconds
[2023-12-16T21:41:20.369+0000] {processor.py:157} INFO - Started process (PID=1008) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:41:20.376+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:41:20.379+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:41:20.378+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:41:20.408+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:41:20.486+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:41:20.486+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:41:20.550+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:41:20.550+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:41:20.588+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.235 seconds
[2023-12-16T21:41:50.697+0000] {processor.py:157} INFO - Started process (PID=1017) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:41:50.698+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:41:50.700+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:41:50.699+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:41:50.724+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:41:50.770+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:41:50.770+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:41:50.810+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:41:50.810+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:41:50.837+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.148 seconds
[2023-12-16T21:42:21.084+0000] {processor.py:157} INFO - Started process (PID=1026) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:42:21.087+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:42:21.088+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:42:21.088+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:42:21.119+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:42:21.178+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:42:21.178+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:42:21.218+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:42:21.218+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:42:21.245+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.177 seconds
[2023-12-16T21:42:51.525+0000] {processor.py:157} INFO - Started process (PID=1035) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:42:51.527+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:42:51.529+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:42:51.528+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:42:51.567+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:42:51.664+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:42:51.664+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:42:51.732+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:42:51.732+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:42:51.789+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.273 seconds
[2023-12-16T21:43:21.953+0000] {processor.py:157} INFO - Started process (PID=1044) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:43:21.955+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:43:21.956+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:43:21.956+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:43:21.990+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:43:22.079+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:43:22.079+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:43:22.120+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:43:22.119+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:43:22.148+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.222 seconds
[2023-12-16T21:43:52.262+0000] {processor.py:157} INFO - Started process (PID=1059) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:43:52.265+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:43:52.269+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:43:52.267+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:43:52.349+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:43:52.429+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:43:52.429+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:43:52.523+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:43:52.523+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:43:52.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.315 seconds
[2023-12-16T21:44:22.656+0000] {processor.py:157} INFO - Started process (PID=1069) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:44:22.658+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:44:22.663+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:44:22.662+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:44:22.694+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:44:22.760+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:44:22.760+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:44:22.809+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:44:22.809+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:44:22.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.200 seconds
[2023-12-16T21:44:53.061+0000] {processor.py:157} INFO - Started process (PID=1077) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:44:53.063+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:44:53.065+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:44:53.064+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:44:53.088+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:44:53.133+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:44:53.133+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:44:53.171+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:44:53.170+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:44:53.197+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.142 seconds
[2023-12-16T21:45:23.351+0000] {processor.py:157} INFO - Started process (PID=1085) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:45:23.353+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:45:23.355+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:45:23.355+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:45:23.375+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:45:23.427+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:45:23.426+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:45:23.463+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:45:23.463+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:45:23.490+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.145 seconds
[2023-12-16T21:45:53.862+0000] {processor.py:157} INFO - Started process (PID=1094) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:45:53.864+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:45:53.866+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:45:53.866+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:45:53.901+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:45:53.976+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:45:53.975+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:45:54.041+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:45:54.041+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:45:54.136+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.285 seconds
[2023-12-16T21:46:24.180+0000] {processor.py:157} INFO - Started process (PID=1102) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:46:24.182+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:46:24.183+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:46:24.183+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:46:24.204+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:46:24.242+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:46:24.242+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:46:24.280+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:46:24.280+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:46:24.313+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.138 seconds
[2023-12-16T21:46:54.508+0000] {processor.py:157} INFO - Started process (PID=1110) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:46:54.510+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:46:54.512+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:46:54.511+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:46:54.533+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:46:54.619+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:46:54.618+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:46:54.677+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:46:54.675+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:46:54.757+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.256 seconds
[2023-12-16T21:47:25.805+0000] {processor.py:157} INFO - Started process (PID=1119) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:47:25.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:47:25.808+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:47:25.808+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:47:25.833+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:47:25.951+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:47:25.951+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:47:26.007+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:47:26.006+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:47:26.042+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.243 seconds
[2023-12-16T21:47:56.156+0000] {processor.py:157} INFO - Started process (PID=1128) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:47:56.159+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:47:56.161+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:47:56.160+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:47:56.184+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:47:56.227+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:47:56.226+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:47:56.281+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:47:56.280+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:47:56.317+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.172 seconds
[2023-12-16T21:48:26.440+0000] {processor.py:157} INFO - Started process (PID=1137) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:48:26.442+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:48:26.443+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:48:26.443+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:48:26.462+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:48:26.505+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:48:26.505+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:48:26.541+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:48:26.541+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:48:26.569+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.134 seconds
[2023-12-16T21:48:56.917+0000] {processor.py:157} INFO - Started process (PID=1145) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:48:56.919+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:48:56.920+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:48:56.920+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:48:56.941+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:48:56.988+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:48:56.988+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:48:57.033+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:48:57.033+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:48:57.065+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.155 seconds
[2023-12-16T21:49:27.190+0000] {processor.py:157} INFO - Started process (PID=1153) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:49:27.192+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:49:27.195+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:49:27.194+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:49:27.221+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:49:27.278+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:49:27.278+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:49:27.316+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:49:27.316+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:49:27.351+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.175 seconds
[2023-12-16T21:49:57.622+0000] {processor.py:157} INFO - Started process (PID=1162) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:49:57.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:49:57.625+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:49:57.625+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:49:57.658+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:49:57.727+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:49:57.727+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:49:57.775+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:49:57.774+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:49:57.813+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.198 seconds
[2023-12-16T21:50:27.941+0000] {processor.py:157} INFO - Started process (PID=1171) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:50:27.942+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:50:27.943+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:50:27.943+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:50:27.968+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:50:28.016+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:50:28.016+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:50:28.058+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:50:28.058+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:50:28.088+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.154 seconds
[2023-12-16T21:50:58.423+0000] {processor.py:157} INFO - Started process (PID=1179) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:50:58.426+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:50:58.427+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:50:58.427+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:50:58.486+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:50:58.589+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:50:58.589+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:50:58.655+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:50:58.654+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:50:58.843+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.443 seconds
[2023-12-16T21:51:29.005+0000] {processor.py:157} INFO - Started process (PID=1187) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:51:29.007+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:51:29.009+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:51:29.008+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:51:29.042+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:51:29.086+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:51:29.085+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:51:29.126+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:51:29.125+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:51:29.152+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.194 seconds
[2023-12-16T21:51:59.203+0000] {processor.py:157} INFO - Started process (PID=1196) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:51:59.205+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:51:59.206+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:51:59.205+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:51:59.225+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:51:59.267+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:51:59.267+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:51:59.305+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:51:59.304+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:51:59.328+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.131 seconds
[2023-12-16T21:52:29.491+0000] {processor.py:157} INFO - Started process (PID=1205) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:52:29.493+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:52:29.494+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:52:29.494+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:52:29.518+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:52:29.564+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:52:29.564+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:52:29.621+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:52:29.621+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:52:29.649+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.164 seconds
[2023-12-16T21:53:00.039+0000] {processor.py:157} INFO - Started process (PID=1214) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:53:00.041+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:53:00.043+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:53:00.043+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:53:00.066+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:53:00.117+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:53:00.116+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:53:00.158+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:53:00.158+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:53:00.188+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.156 seconds
[2023-12-16T21:53:30.490+0000] {processor.py:157} INFO - Started process (PID=1222) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:53:30.491+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:53:30.492+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:53:30.492+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:53:30.516+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:53:30.555+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:53:30.555+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:53:30.593+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:53:30.592+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:53:30.619+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.135 seconds
[2023-12-16T21:54:00.886+0000] {processor.py:157} INFO - Started process (PID=1231) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:54:00.888+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:54:00.889+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:54:00.889+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:54:00.910+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:54:00.962+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:54:00.962+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:54:01.005+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:54:01.004+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:54:01.034+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.154 seconds
[2023-12-16T21:54:31.226+0000] {processor.py:157} INFO - Started process (PID=1240) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:54:31.228+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:54:31.229+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:54:31.228+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:54:31.252+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:54:31.291+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:54:31.291+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:54:31.328+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:54:31.327+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:54:31.354+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.136 seconds
[2023-12-16T21:55:01.582+0000] {processor.py:157} INFO - Started process (PID=1249) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:55:01.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:55:01.586+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:55:01.586+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:55:01.607+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:55:01.656+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:55:01.655+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:55:01.697+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:55:01.697+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:55:01.730+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.154 seconds
[2023-12-16T21:55:31.941+0000] {processor.py:157} INFO - Started process (PID=1258) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:55:31.944+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:55:31.946+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:55:31.946+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:55:32.033+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:55:32.175+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:55:32.174+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:55:32.232+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:55:32.232+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:55:32.263+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.335 seconds
[2023-12-16T21:56:02.332+0000] {processor.py:157} INFO - Started process (PID=1267) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:56:02.334+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:56:02.336+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:56:02.335+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:56:02.360+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:56:02.413+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:56:02.413+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:56:02.459+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:56:02.458+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:56:02.482+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.162 seconds
[2023-12-16T21:56:32.603+0000] {processor.py:157} INFO - Started process (PID=1276) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:56:32.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:56:32.606+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:56:32.606+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:56:32.627+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:56:32.671+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:56:32.671+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:56:32.707+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:56:32.707+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:56:32.732+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.135 seconds
[2023-12-16T21:57:02.826+0000] {processor.py:157} INFO - Started process (PID=1285) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:57:02.828+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:57:02.830+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:57:02.829+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:57:02.851+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:57:02.897+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:57:02.897+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:57:02.934+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:57:02.933+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:57:02.960+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.140 seconds
[2023-12-16T21:57:33.182+0000] {processor.py:157} INFO - Started process (PID=1294) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:57:33.184+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:57:33.186+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:57:33.185+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:57:33.208+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:57:33.253+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:57:33.252+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:57:33.295+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:57:33.294+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:57:33.327+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.151 seconds
[2023-12-16T21:58:03.522+0000] {processor.py:157} INFO - Started process (PID=1303) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:58:03.525+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:58:03.527+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:58:03.526+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:58:03.551+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:58:03.594+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:58:03.593+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:58:03.636+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:58:03.636+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:58:03.663+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.152 seconds
[2023-12-16T21:58:33.788+0000] {processor.py:157} INFO - Started process (PID=1312) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:58:33.789+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:58:33.790+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:58:33.790+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:58:33.838+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:58:33.925+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:58:33.925+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:58:33.962+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:58:33.961+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:58:33.995+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.215 seconds
[2023-12-16T21:59:04.057+0000] {processor.py:157} INFO - Started process (PID=1321) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:59:04.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:59:04.060+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:59:04.060+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:59:04.081+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:59:04.130+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:59:04.129+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:59:04.171+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:59:04.171+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:59:04.209+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.160 seconds
[2023-12-16T21:59:34.335+0000] {processor.py:157} INFO - Started process (PID=1330) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T21:59:34.337+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T21:59:34.338+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:59:34.338+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:59:34.359+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T21:59:34.403+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:59:34.403+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T21:59:34.443+0000] {logging_mixin.py:154} INFO - [2023-12-16T21:59:34.443+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T21:59:34.474+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.146 seconds
[2023-12-16T22:00:04.697+0000] {processor.py:157} INFO - Started process (PID=1339) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T22:00:04.700+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T22:00:04.701+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:00:04.700+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:00:04.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:00:04.775+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:00:04.775+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T22:00:04.816+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:00:04.816+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T22:00:04.842+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.153 seconds
[2023-12-16T22:00:34.936+0000] {processor.py:157} INFO - Started process (PID=1347) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T22:00:34.938+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T22:00:34.939+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:00:34.939+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:00:34.969+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:00:35.025+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:00:35.024+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T22:00:35.063+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:00:35.063+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T22:00:35.123+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.192 seconds
[2023-12-16T22:01:05.198+0000] {processor.py:157} INFO - Started process (PID=1355) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T22:01:05.200+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T22:01:05.201+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:01:05.201+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:01:05.236+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:01:05.303+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:01:05.303+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T22:01:05.343+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:01:05.343+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T22:01:05.375+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.184 seconds
[2023-12-16T22:01:35.890+0000] {processor.py:157} INFO - Started process (PID=1363) to work on /opt/airflow/dags/databricks_test.py
[2023-12-16T22:01:35.896+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/databricks_test.py for tasks to queue
[2023-12-16T22:01:35.905+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:01:35.903+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:01:36.043+0000] {processor.py:839} INFO - DAG(s) dict_keys(['databricks_dag']) retrieved from /opt/airflow/dags/databricks_test.py
[2023-12-16T22:01:36.409+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:01:36.402+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2023-12-16T22:01:36.505+0000] {logging_mixin.py:154} INFO - [2023-12-16T22:01:36.504+0000] {dag.py:3722} INFO - Setting next_dagrun for databricks_dag to 2023-12-16T00:00:00+00:00, run_after=2023-12-17T00:00:00+00:00
[2023-12-16T22:01:36.575+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/databricks_test.py took 0.852 seconds
